---
title: "Newspaper Kiosk Revenue Increase Strategies"
subtitle: "Phase 1: Business & Data Understanding"
author:
- name: "Dominik Gulacsy"
- name: "Attila Szuts"
date: "26/01/2021"
output: pdf_document
abstract: |
  This document detials the process of business and data understanding. It points out data cleaning issues, showcases descriptive and exploratory data analysis, variable transformations and formulates insights gained from them.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

```{r, results='hide', message=FALSE, warning=FALSE}
# Initialize environment --------------------------------------------------
library(dplyr)
library(readxl)
library(ggplot2)
library(ggthemes)
library(tidyverse)
library(janitor)
require(scales)

source("sum_stat.R")

rm(list=ls())

url<-"https://raw.githubusercontent.com/dgulacsy/ddm_kiosk/main/data/clean/"
```

```{r}
# Create sum_stat function
sum_stat <- function( df , var_names , stats , num_obs = TRUE ){
  k <- length( var_names )
  built_in_stat <- c('mean','median','mode','min','max','1st_qu.','3rd_qu',
                     'sd','var','range','iqr')
  do_stat <- intersect( stats , built_in_stat )
  if ( is_empty(do_stat) ){
    stop('Error, no such statistics is implemented! Choose from: mean,median,min,max,1st_qu.,3rd_qu')
  }
  # By default add the number of missing observations and the number of used observations
  m <- length( do_stat )
  if ( num_obs ){
    do_stat <- c( do_stat , "# missing", "# used obs")
  }
  # Create tibble for output
  sum_stat <- as_tibble( matrix( 0 , nrow = m , ncol = k ) , name_repair = "unique" )
  for ( j in 1 : k ) {
    # Get the data for the j'th variable
    var_j <- df[ var_names[ j ] ]
    if ( num_obs ){
      # Count the missing values and add to statistics
      sum_stat[ m + 1 , j ] <- as.integer( sum( is.na( var_j ) ) )
      # Count observations used
      sum_stat[ m + 2 , j ] <- as.integer( sum( !is.na( var_j ) ) )
    }
    # Remove missing values
    var_j <- var_j[ !is.na( var_j ) ]
    # Name the sum_stat's column
    colnames( sum_stat )[ j ] <- var_names[ j ]
    for ( i in 1 : m ) {
      # Central tendency
      if (do_stat[ i ] == "mean"){
        sum_stat[[i,j]] <- mean( var_j )
      } else if (do_stat[ i ] == "median"){
        sum_stat[i,j] <- median( var_j )
      } else if (do_stat[ i ] == "mode"){
        sum_stat[i,j] <- mode( var_j )
      } 
      # Support
      else if (do_stat[ i ] == "min"){
        sum_stat[i,j] <- min( var_j )
      } else if (do_stat[ i ] == "max"){
        sum_stat[i,j] <- max( var_j )
      } 
      # Quartiles
      else if (do_stat[ i ] == "1st_qu."){
        sum_stat[i,j] <- quantile( var_j , probs = 0.25 )
      } else if (do_stat[ i ] == "3rd_qu"){
        sum_stat[i,j] <- quantile( var_j , probs = 0.75)
      } 
      # Dispersion
      else if (do_stat[ i ] == "sd"){
        sum_stat[i,j] <- sd( var_j )
      } else if (do_stat[ i ] == "var"){
        sum_stat[i,j] <- var( var_j )
      } else if (do_stat[ i ] == "range"){
        sum_stat[i,j] <- max( var_j ) - min( var_j )
      } else if (do_stat[ i ] == "iqr"){
        sum_stat[i,j] <- quantile( var_j , probs = 0.75) - quantile( var_j , probs = 0.25)
      } 
    }
  }
  # Finally add a column which contains the requested statistics and relocate to first position
  sum_stat <- sum_stat %>% 
    mutate( statistics = do_stat ) %>% 
    relocate( statistics )
  
  return( sum_stat )
}
```

```{r}
# Read in data ------------------------------------------------------------
turnover <- read_csv(paste0(url,"turnover.csv"))
stores <- read_csv(paste0(url,"stores.csv"))
cost <- read_csv(paste0(url,"cost.csv"))
```

# Data Cleaning
 1. We converted the store_name variable which was stored as numeric value to strings. We did this in case of all 3 dataset (turnover, cost, stores).
 2. We converted nominal variables which were stored as numeric to factor variables to effectively use them later during potential regression modeling.  
 3.  We converted some variables which were stored as character but are indeed numeric.   
 4.  We replaced "#MISSING" values with NAs.
 5.  We flipped the sign of costs
 6.  We divided the Gross Margin and Costs variables with 10,000 to make interpretation easier.

```{r, results='hide'}
# Data cleaning -----------------------------------------------------------
str(stores)
str(turnover)
str(cost)

# convert store_name column to string
turnover$store_name <- as.character(turnover$store_name)
cost$store_name <- as.character(cost$store_name)
stores$store_name <- as.character(stores$store_name)

# convert nominal variables to factors
stores$prague<-as.factor(stores$prague)
stores$price_category<- as.factor(stores$price_category)
stores$lottery_pos<- as.factor(stores$lottery_pos)
stores$pudo_pos<- as.factor(stores$pudo_pos)

# convert numeric variables stored as character to numeric
stores$sunday_opening_hours <- as.numeric(stores$sunday_opening_hours)
stores$real_estate_price_sqm <- as.numeric(stores$real_estate_price_sqm)
stores$x0_50m_avg <- as.numeric(stores$x0_50m_avg)
stores$x50_100m_avg <- as.numeric(stores$x50_100m_avg)
stores$x100_plusm_avg <- as.numeric(stores$x100_plusm_avg)
stores$newspaper_shelves <- as.numeric(stores$newspaper_shelves)
stores$non_newspaper_shelves <- as.numeric(stores$non_newspaper_shelves)

# replace #missing values with NA
stores[stores == "#MISSING"] <- NA

# change sign of costs
cost$costs <- -cost$costs

# change unit of measurement for gm, cost
cost$costs <- cost$costs/10^5
turnover$gm <- turnover$gm/10^5
```


# Data Wrangling
We created 3 major data tables with different aggregation level/granularity:
1. ys - one observation represents one store for one year   
2. ysp - one observation represents one product category for one store for one year   
3. yspm - one observation represents one month for one product category for one store for one year   

```{r}
# Create Year-Store Aggregated Dataset ------------------------------------
# Aggregate data
ys_t <- turnover %>% group_by(year,store_name) %>% 
  summarise(
    gm = sum(gm)
  )

# Join cost and turnover
ys <- left_join(ys_t,cost,by=c("year","store_name"))

# Join df and store data
ys <- left_join(ys,stores, by="store_name")

# Calculate net margin (Net margin = Gross margin - Non-COGS costs)
ys$nm <- ys$gm-ys$costs

# Create Year-Store-ProductCat Aggregated Dataset ------------------------
ysp_t <- turnover %>% group_by(year,store_name, product_category) %>% 
  summarise(
    gm = sum(gm)
  )

ysp <- left_join(ysp_t,stores,by="store_name")

# Create Year-Store-ProductCat-Month level Dataset ------------------------
yspm <- left_join(turnover, stores, by="store_name")

```

# General Descriptive Statistics and Exploratory Data Analysis


```{r}
# General EDA ----------------------------------------------------------------------

stores %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free", ncol = 4) +
  geom_histogram()+
  theme_bw() + 
  scale_fill_wsj()

# Descriptive statistics
dstats<-sum_stat(stores, stores %>%
           keep(is.numeric) %>% 
           colnames(),
         c('mean','median','min','max','1st_qu.','3rd_qu','sd','range'),
         num_obs = F
         ) %>% 
        left_join(sum_stat(turnover,
                           c("gm"),
                           c('mean','median','min','max','1st_qu.','3rd_qu','sd','range'),
                           num_obs = F),
                  by="statistics") %>%
        left_join(sum_stat(cost,
                           c("costs"),
                           c('mean','median','min','max','1st_qu.','3rd_qu','sd','range'),
                           num_obs = F),
                  by="statistics")

# Distributions of GM by product categories and years
ggplot(ysp_t, aes(gm)) + geom_histogram(position = 'dodge') + facet_grid(year ~ product_category,scales = 'free')

# Distributions of GM by year
ggplot(ys, aes(gm)) + geom_histogram(position = 'dodge') + facet_grid(scales = 'free', rows = . ~ year)

# Distributions of Non-COGS Costs by years
ggplot(cost, aes(costs)) + geom_histogram(position = 'dodge') + facet_grid(scales = 'free', rows = . ~ year)

# Cost - Gross Margin Scatterplot
suspicous <- subset(ys, costs<0)

ggplot(ys , aes(x = costs, y = gm)) +
  geom_point() +
  geom_point(data=suspicous, colour="red")+
  geom_text(data=suspicous, label="Error?", vjust=-1, hjust=0.25, color="red")+
  theme_bw() +
  geom_smooth(method="loess" , formula = y ~ x ) +
  labs( title = "Relationship between Non-COGS Costs and Gross Margin",
        y = "Gross Margin (100K)",
        x = "Costs (100K)")+ 
  facet_grid(scales = 'free', rows = . ~ year)
```

We spotted an irregularity in the data which is very likely to be a measurement error. As it can be seen on the right-hand-side chart there is an observation with -0.57 (100K) non-COGS cost. It is unnatural for a cost to be negative.

# Revenue Improvement Strategies | EDA

To focus our attention to store-related patterns rather than stability in time we limited ourselves to the observation belonging to "This year". However, it is advisable to check this stability in a later phase of the analysis.  
```{r}
# Select This year
df <- ysp 
ys <- filter(ys, year=="This year")
ysp <- filter(ysp, year=="This year")
yspm <- filter(yspm, year=="This year")
```


<!-- Dominik -->

## 1. Weekend Opening Hours  

Our first strategic approach to increase revenue is to optimize which stores should be open on Sundays and which should not. In order to get closer to the solution for this we may want to find out whether being open on Sundays results in higher Net Margin.
```{r}
# 1. Weekend Opening Hours ------------------------

ggplot(ys, aes(x = round(sunday_opening_hours,1), y = nm)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method="loess" , formula = y ~ x )+
  labs( title = "Pattern of association between Net Margin and Sunday Opening Hours",
        y = "Net Margin (100K)",
        x = "Sunday Opening Hours")

# Create Sunday open dummy variable
ys$sunday_open<-as.factor(ifelse(ys$sunday_opening_hours>0,1,0))
yspm$sunday_open<-as.factor(ifelse(yspm$sunday_opening_hours>0,1,0))

nm_s <- ys %>% group_by(sunday_open) %>% 
  summarise(
    avg_nm = mean(nm)
  )

ggplot(nm_s, aes(fill=sunday_open, y=avg_nm, x=sunday_open)) + 
  geom_bar(position="dodge", stat="identity")

gm_mps<- yspm %>% group_by(product_category,month,sunday_open) %>% 
  summarise(
    avg_gm = mean(gm)
  )

ggplot(gm_mps, aes(fill=sunday_open, y=avg_gm, x=sunday_open)) + 
  geom_bar(position="dodge", stat="identity") + 
  facet_grid(product_category ~ month,scales = 'free')
```
We can see based on the charts that average Net Margin is higher for stores that are open on Sundays. However we cannot be sure that this is only due to the fact that they are open on Sundays too. Stores may different in many other aspects as well. A linear multiple regression would be helpful to say more on this.

On the last chart we can see that there are significant differences in the effect of being open on Sundays to GM between various product categories which effect also changes meaningfully during the year in some cases. For example there are huge differences in gambling gross margins between stores that are open during Sunday and those that are not. In addition, this difference is even bigger in some months (Nov-Dec). This is is useful to identify which stores should be open on Sundays and which should not using their revenue distribution along product categories.

## 2. Potential benefits of the abolition of PUDO Services   

Our second proposed strategy considers the usage of PUDO services to boost revenue through the development of cross-sales. The ultimate goal would be to identify those stores where PUDO has a high cross-sales potential. Although 88% of the stores already offers PUDO services, in case of the rest it can be even more valuable to choose only those stores to offer this that are more likely to have a higher cross-sales. It also needs to be kept in mind that PUDO may decrease sales of other product categories. For instance, it is not advisable to have heavily-used PUDO services where gambling is intensive since both require quite much place and the staffs' time.
```{r}
# 2. Potential benefits of the abolition of PUDO Services -----------------------------------------------------------

# nrow(ys[ys$pudo_pos==1,])/nrow(ys)

gm_p<- yspm %>% group_by(product_category,month) %>% 
  summarise(
    avg_gm = mean(gm)
  )

ggplot(gm_p, aes(fill=product_category, y=avg_gm, x=product_category)) + 
  geom_bar(position="dodge", stat="identity") + 
  facet_grid(product_category ~ month,scales = 'free')+
  theme(axis.text.x=element_blank(),
        legend.position = "none")

ysp_wide_this <- df %>% filter(year=="This year") %>% 
  select(store_name,product_category,gm) %>% 
  spread(product_category, gm) %>% clean_names()
ysp_wide_last <- df %>% filter(year=="Last year") %>% 
  select(store_name,product_category,gm) %>% 
  spread(product_category, gm) %>% clean_names()

ysp_wide<-left_join(ysp_wide_this,ysp_wide_last,by="store_name")
ysp_wide$dgm_pudo <- (ysp_wide$pudo.x-ysp_wide$pudo.y)/ysp_wide$pudo.y
ysp_wide$dgm_fee_collection <- (ysp_wide$fee_collection.x-ysp_wide$fee_collection.y)/ysp_wide$fee_collection.y
ysp_wide$dgm_sazka <- (ysp_wide$sazka.x-ysp_wide$sazka.y)/ysp_wide$sazka.y
ysp_wide$dgm_newspaper <- (ysp_wide$newspaper.x-ysp_wide$newspaper.y)/ysp_wide$newspaper.y
ysp_wide$dgm_newspaper <- (ysp_wide$newspaper.x-ysp_wide$newspaper.y)/ysp_wide$newspaper.y
ysp_wide$dgm_food_tobacco_and_others <- (ysp_wide$food_tobacco_and_others.x-ysp_wide$food_tobacco_and_others.y)/ysp_wide$food_tobacco_and_others.y

ysp_wide$gm_share_pudo <- ysp_wide$pudo.x/(ysp_wide$fee_collection.x+ysp_wide$food_tobacco_and_others.x+ysp_wide$newspaper.x+ysp_wide$pudo.x+ysp_wide$sazka.x)
ysp_wide$gm_share_sazka <- ysp_wide$sazka.x/(ysp_wide$fee_collection.x+ysp_wide$food_tobacco_and_others.x+ysp_wide$newspaper.x+ysp_wide$pudo.x+ysp_wide$sazka.x)

ggplot(ysp_wide, aes(x = dgm_pudo, y = dgm_sazka)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method="loess" , formula = y ~ x )+
  labs( title = "Pattern of association between PUDO and Gambling Gross Margin Changes",
        y = "Gambling Gross Margin YoY Change(%)",
        x = "PUDO Gross Margin YoY Change (%)")

```
On the first graph we can see that there is significant amount of seasonality present in PUDO sales. It may help in picking the ideal date to launch a marketing campaign aimed at increasing cross-sales. On the second graph we try to explore the relationship between gambling and PUDO sales. We are interested whether there is a negative relationship between the two. In order to filter out the popularity of the stores we used relative change in Gross Margins from last year to this year. We cannot see a meaningful connection between the two but it may worth running some regression models to make sure there is no such pattern.   


## 3. Finding better product combos/mixes  
```{r}
# 3. Finding better product combos/mixes --------
# Calculate newspaper ratio
ys$npr<- ys$newspaper_shelves/(ys$newspaper_shelves+ys$non_newspaper_shelves)

ggplot(ys, aes(x = npr, y = gm)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method="loess" , formula = y ~ x )+
  labs( title = "Pattern of association between Gross Margin and Newspaper focus ratio",
        y = "Gross Margin (100K)",
        x = "Newspaper focus ratio")

ggplot(ys, aes(x = npr, y = nm)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method="loess" , formula = y ~ x )+
  labs( title = "Pattern of association between Gross Margin and Newspaper focus ratio",
        y = "Net Margin (100K)",
        x = "Newspaper focus ratio")
```

Preliminary exploratory analysis suggests that using this strategy will not increase revenue generation.
It seems like there is no strong relationship between the store's newspaper focus and Gross Margin/Net Margin.
A multiple regression would give a better idea whether there is indeed no pattern of association between the two.

<!-- Ati -->

## 4. Underperforming stores
```{r}
# AVG performance YoY
turnover %>% 
  group_by(store_name, product_category, year) %>% 
  summarise(
    avg_gm = mean(gm)
  ) %>%
  ggplot(aes(year, avg_gm)) + geom_boxplot(position = 'dodge') + facet_grid(scales = 'free', rows = . ~ product_category)

# AVG Monthly performance by category 
turnover_mon <- turnover %>% 
  mutate(year = ifelse(year == "This year", "1", "0")) %>% 
  group_by(year, month, product_category) %>% 
  summarise(
    avg_gm = mean(gm)
  ) %>% 
  arrange(year, month, product_category)

grouping <- rep(seq(1, 60), 2)
turnover_mon['grouping'] <- grouping

ggplot(turnover_mon, aes(year, avg_gm)) + geom_point() + facet_grid(scales = 'free', rows = vars(product_category), cols = vars(month)) + geom_line(aes(group = grouping))


# histogram of nm and gm
cost['store_name'] <- as.character(cost$store_name)
turnover['store_name'] <- as.character(turnover$store_name)

df <- turnover %>% 
  left_join(cost, by = c('store_name', 'year')) %>% 
  left_join(stores, by = c('store_name')) %>% 
  mutate(year = fct_recode(as.factor(year), "0" = "Last year", "1" = "This year"))

# calculate NM ( = GM + NCOGS)
df <- df %>% 
  group_by(year, store_name) %>% 
  summarise(
    gm_y = sum(gm)
  ) %>% 
  right_join(df, by = c('year', 'store_name')) %>% 
  mutate(
    nm = gm_y - costs
  )
df_hist <- df %>% group_by(year, store_name) %>% summarise(gm = mean(gm), nm = mean(nm), costs = mean(costs))

df_hist %>% ggplot(aes(nm)) + geom_histogram(bins = 50) + geom_vline(xintercept = median(df_hist$nm), color = "red")
```

## 5. Gambling experience
```{r}
# create total gross-margin and total gross-margin per category
df_g <- df %>% 
  group_by(store_name, product_category, year) %>% 
  summarise(
    total_gm_cat = sum(gm)
  ) %>% 
  right_join(df, by = c("store_name", "product_category", "year"))
df_g <- df_g %>% 
  group_by(store_name, year) %>% 
  summarise(
    total_gm = sum(gm)
  ) %>% 
  right_join(df_g, by = c("store_name", "year"))

# calculate percentage of total gross margin per category
df_g <- df_g %>% 
  group_by(store_name, year, product_category) %>% 
  summarise(
    total_gm_cat_perc = total_gm_cat / total_gm * 100
  ) %>% 
  right_join(df_g, by = c("store_name", "year", "product_category"))

df_g <- df_g[!duplicated(df_g),]


# create average perc of total gm per category per year
prod_cat_becnh <- df_g %>% 
  group_by(year, product_category) %>% 
  summarise(
    avg_perc_gm = mean(total_gm_cat_perc)
  ) %>% 
  arrange(year, desc(avg_perc_gm)) %>% 
  mutate(avg_perc_gm = paste0(round(avg_perc_gm, 2), " %"),
         year = ifelse(year == 0, "Last year", "This year")) %>% 
  rename("Year" = "year", "Product Category" = "product_category", "Average Gross Margin (%)"="avg_perc_gm")
```

## 6. Modelling seasonality
```{r}
turnover_mon <- turnover %>% 
  mutate(year = ifelse(year == "This year", "1", "0")) %>% 
  group_by(year, month, product_category) %>% 
  summarise(
    avg_gm = mean(gm)
  ) %>% 
  arrange(year, month, product_category)

grouping <- rep(seq(1, 60), 2)
turnover_mon['grouping'] <- grouping

ggplot(turnover_mon, aes(year, avg_gm)) + geom_point() + facet_grid(scales = 'free', rows = vars(product_category), cols = vars(month)) + geom_line(aes(group = grouping))



turnover_mon <- turnover_mon %>% 
  arrange(year, product_category, month) 

grouping_mon <- c(rep(1, 12), rep(2, 12), rep(3, 12), rep(4, 12), rep(5, 12), rep(6, 12), rep(7, 12), rep(8, 12), rep(9, 12), rep(10, 12))

turnover_mon['grouping_mon'] <- grouping_mon

turnover_mon %>% ggplot(aes(month, avg_gm)) + facet_grid(rows = vars(product_category), cols = vars(year), scales = "free") + geom_point() + geom_line(aes(group = grouping_mon))

```

# Notes

```{r}

```
